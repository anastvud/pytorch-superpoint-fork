data:
    name: 'my_dataset'      # Display name
    root: 'datasets/barka_dataset'
    dataset: 'my_dataset'   # MUST match the class name in datasets/my_dataset.py
    
    # IMPORTANT: Update this path to where your export step saved the .npz files.
    # Based on your previous command, it should be inside 'barka_dataset_labels/predictions/train'
    labels: 'logs/barka_dataset_labels/predictions'

    
    validation_size: 100    # Number of images to set aside for validation
    
    preprocessing:
        # Resize to match the export step exactly [Height, Width]
        resize: [360, 640] 
        
    gaussian_label:
        enable: false
        sigma: 1.
        
    augmentation:
        photometric:
            enable: true
            enable_train: true
            enable_val: false
            primitives: [
                'random_brightness', 'random_contrast', 'additive_speckle_noise',
                'additive_gaussian_noise', 'additive_shade', 'motion_blur' ]
            params:
                random_brightness: {max_abs_change: 50}
                random_contrast: {strength_range: [0.3, 1.5]}
                additive_gaussian_noise: {stddev_range: [0, 10]}
                additive_speckle_noise: {prob_range: [0, 0.0035]}
                additive_shade:
                    transparency_range: [-0.5, 0.5]
                    kernel_size_range: [100, 150]
                motion_blur: {max_kernel_size: 3} 
        homographic:
            enable: true
            enable_train: true
            enable_val: false
            params:
                translation: true
                rotation: true
                scaling: true
                perspective: true
                scaling_amplitude: 0.2
                perspective_amplitude_x: 0.2
                perspective_amplitude_y: 0.2
                patch_ratio: 0.85
                max_angle: 1.57
                allow_artifacts: true
            valid_border_margin: 3
            
    warped_pair:
        enable: true
        params:
            translation: true
            rotation: true
            scaling: true
            perspective: true
            scaling_amplitude: 0.2
            perspective_amplitude_x: 0.2
            perspective_amplitude_y: 0.2
            patch_ratio: 0.85
            max_angle: 1.57
            allow_artifacts: true
        valid_border_margin: 3

front_end_model: 'Train_model_heatmap'


model:
    # name: 'magic_point'
    # name: 'SuperPointNet_heatmap'
    name: 'SuperPointNet_gauss2'
    params: {
    }
    detector_loss:
        loss_type: 'softmax'


    batch_size: 8 # 32
    eval_batch_size: 8 # 32
    learning_rate: 0.0001 # 0.0001
    detection_threshold: 0.015 # 0.015
    lambda_loss: 1 # 1
    nms: 4
    dense_loss:
        enable: false
        params:
            descriptor_dist: 4 # 4, 7.5
            lambda_d: 800 # 800
    sparse_loss:
        enable: true
        params:
            num_matching_attempts: 1000
            num_masked_non_matches_per_match: 100
            lamda_d: 1
            dist: 'cos'
            method: '2d'
    other_settings: 'train 2d, gauss 0.2'
    subpixel:
        enable: false
        params:
            subpixel_channel: 2
        settings: 'predict flow directly'
        loss_func: 'subpixel_loss_no_argmax' # subpixel_loss, subpixel_loss_no_argmax


retrain: true # Set to true to allow fine-tuning
reset_iter: true # Reset the iteration counter

# Start from the synthetic weights (the same ones you used for export)
pretrained: 'logs/superpoint_coco_heat2_0/checkpoints/superPointNet_170000_checkpoint.pth.tar'

train_iter: 200000 # Total training steps (you can stop manually with Ctrl+C)
validation_interval: 500 
train_show_interval: 200 # Visualize progress often